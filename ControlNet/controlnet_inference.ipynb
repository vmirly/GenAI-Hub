{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3918a874",
   "metadata": {},
   "source": [
    "ControlNet\n",
    "=====\n",
    "\n",
    "**Adding Conditional Control to Text-to-Image Diffusion Models**\n",
    "\n",
    " * Paper: https://arxiv.org/pdf/2302.05543\n",
    "\n",
    "![ControlNet Overview](../assets/controlnet_overview.png)\n",
    "\n",
    "![ControlNet Overview](../assets/controlnet_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d51147",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/lllyasviel/ControlNet.git ControlNet_repo\n",
    "\n",
    "sed -i 's,name: control$,name: controlnet,g' ControlNet_repo/environment.yaml\n",
    "\n",
    "# upgrade transformers to be able to download 'openai/clip-vit-large-patch14'\n",
    "pip install --upgrade transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ace113",
   "metadata": {},
   "source": [
    " * Download the model using huggingface hub\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "repo_id = \"lllyasviel/ControlNet\"\n",
    "filenames = [\n",
    "    \"models/control_sd15_canny.pth\",\n",
    "    \"models/control_sd15_depth.pth\",\n",
    "    \"models/control_sd15_seg.pth\"\n",
    "]\n",
    "\n",
    "save_dir = \"./models\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for filename in filenames:\n",
    "    cached_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    \n",
    "    # Copy to your custom directory\n",
    "    dst_path = os.path.join(save_dir, os.path.basename(filename))\n",
    "    shutil.copyfile(cached_path, dst_path)\n",
    "    print(f\"Copied to: {dst_path}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad67ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/controlnet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"ControlNet_repo\")\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.canny import CannyDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "\n",
    "from share import *\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77465c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [ControlNet_repo/models/cldm_v15.yaml]\n",
      "Loaded state_dict from [./models/control_sd15_canny.pth]\n",
      "Missing keys: []\n",
      "Unexpected keys: ['cond_stage_model.transformer.text_model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = create_model('ControlNet_repo/models/cldm_v15.yaml').cpu()\n",
    "#model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cuda'))\n",
    "state_dict = load_state_dict('./models/control_sd15_canny.pth', location=device)\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Missing keys: {missing_keys}\")\n",
    "print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "model = model.to(device)\n",
    "ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc61e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(\n",
    "        input_image,\n",
    "        edge_map,\n",
    "        prompt,\n",
    "        a_prompt,\n",
    "        n_prompt,\n",
    "        num_samples=1,\n",
    "        image_resolution=512,\n",
    "        ddim_steps=20,\n",
    "        guess_mode=False,\n",
    "        strength=1.0,\n",
    "        scale=9.0,\n",
    "        seed=1,\n",
    "        eta=0.0\n",
    "    ):\n",
    "    with torch.no_grad():\n",
    "        img = resize_image(HWC3(input_image), image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        edge_map = resize_image(HWC3(edge_map), image_resolution)\n",
    "        edge_map = HWC3(edge_map)\n",
    "\n",
    "        control = torch.from_numpy(edge_map.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
    "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(\n",
    "            ddim_steps, num_samples,\n",
    "            shape, cond, verbose=False, eta=eta,\n",
    "            unconditional_guidance_scale=scale,\n",
    "            unconditional_conditioning=un_cond\n",
    "        )\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (\n",
    "            einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5\n",
    "        ).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "    return [255 - edge_map] + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfad636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 72, 64), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  70%|███████   | 14/20 [00:03<00:01,  4.12it/s]"
     ]
    }
   ],
   "source": [
    "image_path = \"../samples/plants.jpg\"\n",
    "input_image = cv2.imread(image_path)\n",
    "\n",
    "# generate edge map using Canny edge detector\n",
    "detector = CannyDetector()\n",
    "detected_map = detector(input_image, low_threshold=100, high_threshold=200)\n",
    "\n",
    "res = process(\n",
    "    input_image=input_image,\n",
    "    edge_map=detected_map,\n",
    "    prompt=\"A photo of a plant\",\n",
    "    a_prompt=\"Aesthetic\",\n",
    "    n_prompt=\"Blurry\",\n",
    "    num_samples=1,\n",
    "    image_resolution=512,\n",
    "    ddim_steps=20,\n",
    "    guess_mode=False,\n",
    "    strength=1.0,\n",
    "    scale=9.0,\n",
    "    seed=1,\n",
    "    eta=0.0\n",
    ")\n",
    "\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ad17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "controlnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
