{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2dc12e",
   "metadata": {},
   "source": [
    "LLaMA 2\n",
    "===\n",
    "\n",
    "**Llama 2: Open Foundation and Fine-Tuned Chat Models**\n",
    "\n",
    " * Paper: https://arxiv.org/abs/2307.09288\n",
    "\n",
    "![Llama 2 Overview](../assets/llama2_overview.png)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/facebookresearch/llama/ llama_repo\n",
    "```\n",
    "\n",
    "Folder structure:\n",
    "```bash\n",
    "llama_repo/\n",
    "├── CODE_OF_CONDUCT.md\n",
    "├── CONTRIBUTING.md\n",
    "├── download.sh\n",
    "├── example_chat_completion.py\n",
    "├── example_text_completion.py\n",
    "├── LICENSE\n",
    "├── llama\n",
    "│   ├── generation.py\n",
    "│   ├── __init__.py\n",
    "│   ├── model.py\n",
    "│   └── tokenizer.py\n",
    "├── MODEL_CARD.md\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── Responsible-Use-Guide.pdf\n",
    "├── setup.py\n",
    "├── UPDATES.md\n",
    "└── USE_POLICY.md\n",
    "```\n",
    "\n",
    "* Insrallation\n",
    "```bash\n",
    "pip install torch\n",
    "cd llama_repo\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1febd4",
   "metadata": {},
   "source": [
    " * Getting the model\n",
    "\n",
    "# Feel out the request form at https://www.llama.com/llama-downloads/\n",
    "\n",
    "![Llama Request Form](../assets/llama_request_form.png)\n",
    "\n",
    "```bash\n",
    "pip install llama-stack -U\n",
    "\n",
    "llama model list --show-all\n",
    "\n",
    "llama download --source huggingface --model-id Llama-2-7b\n",
    "# Successfully downloaded model to /home/pyml/.llama/checkpoints/Llama-2-7b\n",
    "```\n",
    "\n",
    " * For more information, please visit https://github.com/meta-llama/llama-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1d5dc",
   "metadata": {},
   "source": [
    "## Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5f256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fire\n",
    "\n",
    "from llama import Llama\n",
    "from typing import List\n",
    "\n",
    "#ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set\n",
    "# ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable WORLD_SIZE expected, but not set\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "#ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set\n",
    "#ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_PORT expected, but not set\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1c0c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/llama/lib/python3.12/site-packages/torch/__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.99 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama.generation.Llama at 0x7fd433d69220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt dir: ~/.llama/checkpoints/Llama-2-7b/\n",
    "ckpt_dir = os.path.join(os.path.expanduser(\"~\"), \".llama\", \"checkpoints\", \"Llama-2-7b\")\n",
    "tokenizer_path = os.path.join(ckpt_dir, \"tokenizer.model\")\n",
    "max_seq_len = 128\n",
    "max_batch_size = 1\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e9b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
