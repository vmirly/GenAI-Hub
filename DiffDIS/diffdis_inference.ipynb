{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d3651f",
   "metadata": {},
   "source": [
    "DiffDIS\n",
    "====\n",
    "\n",
    "**High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity**\n",
    "\n",
    " * Paper: https://arxiv.org/abs/2410.10105\n",
    "\n",
    "![DiffDIS Overview](../assets/diffdis_overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c1782",
   "metadata": {},
   "source": [
    "\n",
    "! Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/qianyu-dlut/DiffDIS.git\n",
    "cd DiffDIS\n",
    "\n",
    "pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install -r requirements.txt\n",
    "pip install -e diffusers-0.30.2/\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    " * Download pre-trained model (sd-ultra)\n",
    "```python\n",
    "...: from huggingface_hub import snapshot_download\n",
    "...: \n",
    "...: # Replace this with your target directory\n",
    "...: local_dir = \"./models/sd-turbo\"\n",
    "...: \n",
    "...: # Download entire repository snapshot\n",
    "...: snapshot_download(\n",
    "...:     repo_id=\"stabilityai/sd-turbo\",\n",
    "...:     local_dir=local_dir,\n",
    "...:     local_dir_use_symlinks=False  # Set to False to copy files instead of symlinks\n",
    "...: )\n",
    "```\n",
    "\n",
    " * Download the checkpoint from [Google Drive](https://drive.google.com/drive/folders/1NKmUbn9BiV7xYy_1c2khIBAuOQNuYAdR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c15db",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install gdown\n",
    "\n",
    "# Right click on the file and get the link\n",
    "\n",
    "gdown https://drive.google.com/uc?id=FILE_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13024b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/diffdis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "sys.path.append(\"DiffDIS-repo\")\n",
    "from core.diffdis_pipeline import DiffDISPipeline\n",
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel_diffdis,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "\n",
    "from utils.seed_all import seed_all \n",
    "from utils.utils import check_mkdir\n",
    "from utils.config import diste1,diste2,diste3,diste4,disvd\n",
    "from utils.image_util import resize_res\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fddc6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_seperate = False\n",
    "    \n",
    "checkpoint_path = \"DiffDIS-repo/checkpoints/\"\n",
    "pretrained_model_path = \"DiffDIS-repo/models/sd-turbo\"\n",
    "      \n",
    "# Diffusion denoising steps, more steps results in higher accuracy but slower inference speed\n",
    "denoise_steps = 1\n",
    "# Number of predictions to be ensembled, more inference gives better results but runs slower.\n",
    "ensemble_size = 1\n",
    "# Run with half-precision (16-bit float), might lead to suboptimal result.\n",
    "half_precision = False\n",
    "# Maximum resolution of processing. 0 for using input image resolution. Default: 1024.\n",
    "processing_res = 1024\n",
    "# Match input resolution for output depth. Default: False.\n",
    "match_input_res = True\n",
    "# Random seed for reproducibility\n",
    "seed = None\n",
    "# Inference batch size. Default: 0 (will be set automatically).\n",
    "batch_size = 1\n",
    "\n",
    "match_output_input_res = False\n",
    "\n",
    "    \n",
    "# -------------------- Preparation --------------------\n",
    "# Random seed\n",
    "if seed is None:\n",
    "    import time\n",
    "    seed = int(time.time())\n",
    "seed_all(seed)\n",
    "\n",
    "# -------------------- Device --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import ttach as tta\n",
    "transforms = tta.Compose(\n",
    "[\n",
    "    tta.HorizontalFlip(),\n",
    "    tta.Scale(scales=[0.75, 1, 1.25], interpolation='bilinear', align_corners=False),\n",
    "])\n",
    "    \n",
    "dtype = torch.float16 if half_precision else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d536d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seperated Modules\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Model --------------------\n",
    "    \n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_path, subfolder='vae'\n",
    ")\n",
    "scheduler = DDPMScheduler.from_pretrained(\n",
    "    pretrained_model_path, subfolder='scheduler'\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_path, subfolder='text_encoder'\n",
    ")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_path, subfolder='tokenizer'\n",
    ")\n",
    "unet = UNet2DConditionModel_diffdis.from_pretrained(\n",
    "    checkpoint_path, subfolder=\"unet\",\n",
    "    in_channels=8, sample_size=96,\n",
    "    low_cpu_mem_usage=False,\n",
    "    ignore_mismatched_sizes=False,\n",
    "    class_embed_type='projection',\n",
    "    projection_class_embeddings_input_dim=4,\n",
    "    mid_extra_cross=True,\n",
    "    mode = 'DBIA', use_swci = True\n",
    ")\n",
    "\n",
    "pipe = DiffDISPipeline(\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    scheduler=scheduler,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Using Seperated Modules\")\n",
    "    \n",
    "\n",
    "pipe = pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73dad64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Inference --------------------\n",
    "def run_inference(input_image_pil):\n",
    "    w_,h_ = input_image_pil.size\n",
    "    img_resize = resize_res(input_image_pil, resolution=processing_res)\n",
    "\n",
    "    input_image = img_resize.convert(\"RGB\")\n",
    "    image = np.array(input_image)\n",
    "    rgb = np.transpose(image,(2,0,1))\n",
    "    rgb_norm = rgb / 255.0 * 2.0 - 1.0\n",
    "    rgb_norm = torch.from_numpy(rgb_norm).to(device)\n",
    "    rgb_norm = rgb_norm.to(device).float().unsqueeze(0)\n",
    "    mask_m = []\n",
    "    mask_e = []\n",
    "    for transformer in transforms:\n",
    "        img_resize = transformer.augment_image(rgb_norm)\n",
    "        with torch.no_grad():\n",
    "            pipe_out_m, pipe_out_e = pipe(\n",
    "                img_resize,\n",
    "                denosing_steps=denoise_steps,\n",
    "                ensemble_size=ensemble_size,\n",
    "                processing_res=processing_res,\n",
    "                match_input_res=match_input_res,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "        deaug_mask_m = transformer.deaugment_mask(pipe_out_m.unsqueeze(0).unsqueeze(0))\n",
    "        mask_m.append(deaug_mask_m)\n",
    "        deaug_mask_e = transformer.deaugment_mask(pipe_out_e.unsqueeze(0).unsqueeze(0))\n",
    "        mask_e.append(deaug_mask_e)\n",
    "\n",
    "\n",
    "    prediction_m = torch.mean(torch.stack(mask_m, dim=0), dim=0)\n",
    "    prediction_e = torch.mean(torch.stack(mask_e, dim=0), dim=0)\n",
    "\n",
    "    prediction_m = to_pil(prediction_m.data.squeeze(0).cpu())\n",
    "    prediction_m = prediction_m.resize((w_, h_), Image.BILINEAR)\n",
    "    \n",
    "    prediction_e = to_pil(prediction_e.data.squeeze(0).cpu())\n",
    "    prediction_e = prediction_e.resize((w_, h_), Image.BILINEAR)\n",
    "\n",
    "    return prediction_m, prediction_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14b8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068, 1137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1068, 1137), (1068, 1137))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"../samples/plants.jpg\"\n",
    "input_image_pil = Image.open(image_path)\n",
    "print(input_image_pil.size)\n",
    "\n",
    "pred_m, pred_e = run_inference(input_image_pil)\n",
    "\n",
    "pred_m.size, pred_e.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25643bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffdis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
