{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6517564",
   "metadata": {},
   "source": [
    "OVSeg\n",
    "====\n",
    "\n",
    " **Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP**\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2210.04150\n",
    "\n",
    "![OVSeg Overview](../assets/ovseg_overview.jpg)\n",
    "![OVSeg Prompting](../assets/ovseg_prompt.jpg)\n",
    "![OVSeg Two-Stage](../assets/ovseg_twostage.jpg)\n",
    "![OVSeg Image-Text](../assets/ovseg_imgtextmatching.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006806ea",
   "metadata": {},
   "source": [
    " * Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/facebookresearch/ov-seg.git OVSeg_repo\n",
    "\n",
    "cd OVSeg_repo\n",
    "\n",
    "conda create --name ovseg python=3.8 -y\n",
    "conda activate ovseg\n",
    "conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n",
    "\n",
    "\n",
    "cd third_party/CLIP\n",
    "python -m pip install -Ue .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5f2c4",
   "metadata": {},
   "source": [
    "* Download model: https://drive.google.com/file/d/1cn-ohxgXDrDfkzC1QdO-fi8IjbjXmgKy/view\n",
    "\n",
    "```bash\n",
    "pip install -U gdown\n",
    "```\n",
    "\n",
    "```python\n",
    "gdown --id 1cn-ohxgXDrDfkzC1QdO-fi8IjbjXmgKy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7be289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import tqdm\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "sys.path.append(\"OVSeg_repo\")\n",
    "from open_vocab_seg import add_ovseg_config\n",
    "#from open_vocab_seg.utils import VisualizationDemo\n",
    "from open_vocab_seg import utils\n",
    "\n",
    "# constants\n",
    "WINDOW_NAME = \"Open vocabulary segmentation\"\n",
    "\n",
    "\n",
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    # for poly lr schedule\n",
    "    add_deeplab_config(cfg)\n",
    "    add_ovseg_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def parse_args(argv):\n",
    "    parser = argparse.ArgumentParser(description=\"Detectron2 demo for open vocabulary segmentation\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=\"configs/ovseg_swinB_vitL_demo.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_path\", type=str,\n",
    "        help=\"or a single glob pattern such as 'directory/*.jpg'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class-names\",\n",
    "        nargs=\"+\",\n",
    "        help=\"A list of user-defined class_names\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        help=\"A file or directory to save output visualizations. \"\n",
    "        \"If not given, will show output in an OpenCV window.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n",
    "        default=[],\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    return parser.parse_args(argv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c5bdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/01 23:57:25 fvcore.common.config]: \u001b[0mLoading config OVSeg_repo/configs/ovseg_swinB_vitL_demo.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CfgNode({'VERSION': 2, 'MODEL': CfgNode({'LOAD_PROPOSALS': False, 'MASK_ON': False, 'KEYPOINT_ON': False, 'DEVICE': 'cuda', 'META_ARCHITECTURE': 'OVSegDEMO', 'WEIGHTS': 'ovseg_swinbase_vitL14_ft_mpt.pth', 'PIXEL_MEAN': [123.675, 116.28, 103.53], 'PIXEL_STD': [58.395, 57.12, 57.375], 'BACKBONE': CfgNode({'NAME': 'D2SwinTransformer', 'FREEZE_AT': 0}), 'FPN': CfgNode({'IN_FEATURES': [], 'OUT_CHANNELS': 256, 'NORM': '', 'FUSE_TYPE': 'sum'}), 'PROPOSAL_GENERATOR': CfgNode({'NAME': 'RPN', 'MIN_SIZE': 0}), 'ANCHOR_GENERATOR': CfgNode({'NAME': 'DefaultAnchorGenerator', 'SIZES': [[32, 64, 128, 256, 512]], 'ASPECT_RATIOS': [[0.5, 1.0, 2.0]], 'ANGLES': [[-90, 0, 90]], 'OFFSET': 0.0}), 'RPN': CfgNode({'HEAD_NAME': 'StandardRPNHead', 'IN_FEATURES': ['res4'], 'BOUNDARY_THRESH': -1, 'IOU_THRESHOLDS': [0.3, 0.7], 'IOU_LABELS': [0, -1, 1], 'BATCH_SIZE_PER_IMAGE': 256, 'POSITIVE_FRACTION': 0.5, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0), 'SMOOTH_L1_BETA': 0.0, 'LOSS_WEIGHT': 1.0, 'PRE_NMS_TOPK_TRAIN': 12000, 'PRE_NMS_TOPK_TEST': 6000, 'POST_NMS_TOPK_TRAIN': 2000, 'POST_NMS_TOPK_TEST': 1000, 'NMS_THRESH': 0.7, 'CONV_DIMS': [-1]}), 'ROI_HEADS': CfgNode({'NAME': 'Res5ROIHeads', 'NUM_CLASSES': 80, 'IN_FEATURES': ['res4'], 'IOU_THRESHOLDS': [0.5], 'IOU_LABELS': [0, 1], 'BATCH_SIZE_PER_IMAGE': 512, 'POSITIVE_FRACTION': 0.25, 'SCORE_THRESH_TEST': 0.05, 'NMS_THRESH_TEST': 0.5, 'PROPOSAL_APPEND_GT': True}), 'ROI_BOX_HEAD': CfgNode({'NAME': '', 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0), 'SMOOTH_L1_BETA': 0.0, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2', 'NUM_FC': 0, 'FC_DIM': 1024, 'NUM_CONV': 0, 'CONV_DIM': 256, 'NORM': '', 'CLS_AGNOSTIC_BBOX_REG': False, 'TRAIN_ON_PRED_BOXES': False}), 'ROI_BOX_CASCADE_HEAD': CfgNode({'BBOX_REG_WEIGHTS': ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0)), 'IOUS': (0.5, 0.6, 0.7)}), 'ROI_MASK_HEAD': CfgNode({'NAME': 'MaskRCNNConvUpsampleHead', 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'NUM_CONV': 0, 'CONV_DIM': 256, 'NORM': '', 'CLS_AGNOSTIC_MASK': False, 'POOLER_TYPE': 'ROIAlignV2'}), 'ROI_KEYPOINT_HEAD': CfgNode({'NAME': 'KRCNNConvDeconvUpsampleHead', 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'CONV_DIMS': (512, 512, 512, 512, 512, 512, 512, 512), 'NUM_KEYPOINTS': 17, 'MIN_KEYPOINTS_PER_IMAGE': 1, 'NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS': True, 'LOSS_WEIGHT': 1.0, 'POOLER_TYPE': 'ROIAlignV2'}), 'SEM_SEG_HEAD': CfgNode({'NAME': 'OpenVocabMaskFormerHead', 'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'], 'IGNORE_VALUE': 255, 'NUM_CLASSES': 171, 'CONVS_DIM': 256, 'COMMON_STRIDE': 4, 'NORM': 'GN', 'LOSS_WEIGHT': 1.0, 'LOSS_TYPE': 'hard_pixel_mining', 'PROJECT_FEATURES': ['res2'], 'PROJECT_CHANNELS': [48], 'ASPP_CHANNELS': 256, 'ASPP_DILATIONS': [6, 12, 18], 'ASPP_DROPOUT': 0.1, 'USE_DEPTHWISE_SEPARABLE_CONV': False, 'MASK_DIM': 256, 'TRANSFORMER_ENC_LAYERS': 0, 'PIXEL_DECODER_NAME': 'BasePixelDecoder', 'EMBEDDING_DIM': 768, 'EMBED_HIDDEN_DIM': 1024, 'EMBED_LAYERS': 2}), 'PANOPTIC_FPN': CfgNode({'INSTANCE_LOSS_WEIGHT': 1.0, 'COMBINE': CfgNode({'ENABLED': True, 'OVERLAP_THRESH': 0.5, 'STUFF_AREA_LIMIT': 4096, 'INSTANCES_CONFIDENCE_THRESH': 0.5})}), 'RETINANET': CfgNode({'NUM_CLASSES': 80, 'IN_FEATURES': ['p3', 'p4', 'p5', 'p6', 'p7'], 'NUM_CONVS': 4, 'IOU_THRESHOLDS': [0.4, 0.5], 'IOU_LABELS': [0, -1, 1], 'PRIOR_PROB': 0.01, 'SCORE_THRESH_TEST': 0.05, 'TOPK_CANDIDATES_TEST': 1000, 'NMS_THRESH_TEST': 0.5, 'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0), 'FOCAL_LOSS_GAMMA': 2.0, 'FOCAL_LOSS_ALPHA': 0.25, 'SMOOTH_L1_LOSS_BETA': 0.1, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'NORM': ''}), 'RESNETS': CfgNode({'DEPTH': 50, 'OUT_FEATURES': ['res4'], 'NUM_GROUPS': 1, 'NORM': 'FrozenBN', 'WIDTH_PER_GROUP': 64, 'STRIDE_IN_1X1': True, 'RES5_DILATION': 1, 'RES2_OUT_CHANNELS': 256, 'STEM_OUT_CHANNELS': 64, 'DEFORM_ON_PER_STAGE': [False, False, False, False], 'DEFORM_MODULATED': False, 'DEFORM_NUM_GROUPS': 1, 'RES4_DILATION': 1, 'RES5_MULTI_GRID': [1, 2, 4], 'STEM_TYPE': 'deeplab'}), 'MASK_FORMER': CfgNode({'DEEP_SUPERVISION': True, 'NO_OBJECT_WEIGHT': 0.1, 'DICE_WEIGHT': 1.0, 'MASK_WEIGHT': 20.0, 'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 2048, 'ENC_LAYERS': 0, 'DEC_LAYERS': 6, 'PRE_NORM': False, 'HIDDEN_DIM': 256, 'NUM_OBJECT_QUERIES': 100, 'TRANSFORMER_IN_FEATURE': 'res5', 'ENFORCE_INPUT_PROJ': False, 'TEST': CfgNode({'PANOPTIC_ON': False, 'OBJECT_MASK_THRESHOLD': 0.0, 'OVERLAP_THRESHOLD': 0.0, 'SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE': False}), 'SIZE_DIVISIBILITY': 32}), 'SWIN': CfgNode({'PRETRAIN_IMG_SIZE': 384, 'PATCH_SIZE': 4, 'EMBED_DIM': 128, 'DEPTHS': [2, 2, 18, 2], 'NUM_HEADS': [4, 8, 16, 32], 'WINDOW_SIZE': 12, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'QK_SCALE': None, 'NORM_INDICES': None, 'PROJECTION': False, 'PROJECT_DIM': 256, 'DROP_RATE': 0.0, 'ATTN_DROP_RATE': 0.0, 'DROP_PATH_RATE': 0.3, 'APE': False, 'PATCH_NORM': True, 'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5']}), 'CLIP_ADAPTER': CfgNode({'TEXT_TEMPLATES': 'vild', 'PREDEFINED_PROMPT_TEMPLATES': ['a photo of a {}.'], 'PROMPT_CHECKPOINT': '', 'CLIP_MODEL_NAME': 'ViT-L/14', 'MASK_FILL': 'mean', 'MASK_EXPAND_RATIO': 1.0, 'MASK_THR': 0.35, 'MASK_MATTING': False, 'REGION_RESIZED': True, 'CLIP_ENSEMBLE': True, 'CLIP_ENSEMBLE_WEIGHT': 0.0, 'MASK_PROMPT_DEPTH': 3, 'MASK_PROMPT_FWD': True})}), 'INPUT': CfgNode({'MIN_SIZE_TRAIN': (320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1088, 1152, 1216, 1280), 'MIN_SIZE_TRAIN_SAMPLING': 'choice', 'MAX_SIZE_TRAIN': 2560, 'MIN_SIZE_TEST': 640, 'MAX_SIZE_TEST': 2560, 'RANDOM_FLIP': 'horizontal', 'CROP': CfgNode({'ENABLED': True, 'TYPE': 'absolute', 'SIZE': [640, 640], 'SINGLE_CATEGORY_MAX_AREA': 1.0}), 'FORMAT': 'RGB', 'MASK_FORMAT': 'polygon', 'DATASET_MAPPER_NAME': 'mask_former_semantic', 'COLOR_AUG_SSD': True, 'SIZE_DIVISIBILITY': 640}), 'DATASETS': CfgNode({'TRAIN': ('coco_2017_train_stuff_sem_seg',), 'PROPOSAL_FILES_TRAIN': (), 'PRECOMPUTED_PROPOSAL_TOPK_TRAIN': 2000, 'TEST': ('ade20k_sem_seg_val',), 'PROPOSAL_FILES_TEST': (), 'PRECOMPUTED_PROPOSAL_TOPK_TEST': 1000, 'SAMPLE_PER_CLASS': -1, 'SAMPLE_SEED': 0}), 'DATALOADER': CfgNode({'NUM_WORKERS': 4, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler', 'REPEAT_THRESHOLD': 0.0, 'FILTER_EMPTY_ANNOTATIONS': True}), 'SOLVER': CfgNode({'LR_SCHEDULER_NAME': 'WarmupMultiStepLR', 'MAX_ITER': 120000, 'BASE_LR': 6e-05, 'MOMENTUM': 0.9, 'NESTEROV': False, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_NORM': 0.0, 'GAMMA': 0.1, 'STEPS': (30000,), 'WARMUP_FACTOR': 1e-06, 'WARMUP_ITERS': 1500, 'WARMUP_METHOD': 'linear', 'CHECKPOINT_PERIOD': 5000, 'IMS_PER_BATCH': 32, 'REFERENCE_WORLD_SIZE': 0, 'BIAS_LR_FACTOR': 1.0, 'WEIGHT_DECAY_BIAS': None, 'CLIP_GRADIENTS': CfgNode({'ENABLED': True, 'CLIP_TYPE': 'full_model', 'CLIP_VALUE': 0.01, 'NORM_TYPE': 2.0}), 'AMP': CfgNode({'ENABLED': False}), 'POLY_LR_POWER': 0.9, 'POLY_LR_CONSTANT_ENDING': 0.0, 'TEST_IMS_PER_BATCH': 1, 'WEIGHT_DECAY_EMBED': 0.0, 'OPTIMIZER': 'ADAMW', 'BACKBONE_MULTIPLIER': 1.0}), 'TEST': CfgNode({'EXPECTED_RESULTS': [], 'EVAL_PERIOD': 5000, 'KEYPOINT_OKS_SIGMAS': [], 'DETECTIONS_PER_IMAGE': 100, 'AUG': CfgNode({'ENABLED': False, 'MIN_SIZES': (256, 384, 512, 640, 768, 896), 'MAX_SIZE': 3584, 'FLIP': True}), 'PRECISE_BN': CfgNode({'ENABLED': False, 'NUM_ITER': 200}), 'SLIDING_WINDOW': False, 'SLIDING_TILE_SIZE': 224, 'SLIDING_OVERLAP': 0.6666666666666666, 'DENSE_CRF': False}), 'OUTPUT_DIR': './output', 'SEED': -1, 'CUDNN_BENCHMARK': False, 'VIS_PERIOD': 0, 'GLOBAL': CfgNode({'HACK': 1.0}), 'WANDB': CfgNode({'PROJECT': 'open_vocab_seg', 'NAME': None})})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argv = [\n",
    "    \"--input_path\", \"plants_small.jpg\",\n",
    "    \"--class-names\", \"plant,leaf,pot,vase\",\n",
    "    \"--config-file\", \"OVSeg_repo/configs/ovseg_swinB_vitL_demo.yaml\",\n",
    "    \"--opts\", \"MODEL.WEIGHTS\", \"ovseg_swinbase_vitL14_ft_mpt.pth\",\n",
    "]\n",
    "\n",
    "args = parse_args(argv)\n",
    "cfg = setup_cfg(args)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b9775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 267, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img_orig = Image.open(\"../samples/plants.jpg\")\n",
    "w, h = img_orig.size\n",
    "img_orig.resize((int(w/4), int(h/4))).save(\"plants_small.jpg\")\n",
    "\n",
    "img = read_image(\"plants_small.jpg\", format=\"BGR\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9589f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d28c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/02 00:03:57 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from ovseg_swinbase_vitL14_ft_mpt.pth ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "metadata = MetadataCatalog.get(\n",
    "    cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
    ")\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "instance_mode = ColorMode.IMAGE\n",
    "\n",
    "predictor = utils.predictor.OVSegPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be1307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
