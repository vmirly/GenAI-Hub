{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f042944d",
   "metadata": {},
   "source": [
    "Seg-Zero\n",
    "====\n",
    "\n",
    "**Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement**\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2503.06520\n",
    "\n",
    "\n",
    "![Seg-Zero Overview](../assets/segzero_arch.jpg)\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dvlab-research/Seg-Zero.git SegZero_repo\n",
    "cd SegZero_repo\n",
    "conda create -n segzero python=3.12 -y\n",
    "conda activate segzero\n",
    "pip install torch==2.6.0 torchvision==0.21.0\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "## Download models\n",
    "\n",
    "```bash\n",
    "mkdir pretrained_models\n",
    "cd pretrained_models\n",
    "git lfs install\n",
    "git clone https://huggingface.co/Ricky06662/VisionReasoner-7B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/segzero/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pdb\n",
    "import re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48659ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_model_path = \"SegZero_repo/pretrained_models/VisionReasoner-7B\"\n",
    "segmentation_model_path = \"facebook/sam2-hiera-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9119ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question:  Which plant is taller?\n"
     ]
    }
   ],
   "source": [
    "reasoning_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    reasoning_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "segmentation_model = SAM2ImagePredictor.from_pretrained(segmentation_model_path)\n",
    "\n",
    "reasoning_model.eval()\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(reasoning_model_path, padding_side=\"left\")\n",
    "\n",
    "print(\"User question: \", text)\n",
    "\n",
    "QUESTION_TEMPLATE = \\\n",
    "    \"Please find \\\"{Question}\\\" with bboxs and points.\" \\\n",
    "    \"Compare the difference between object(s) and find the most closely matched object(s).\" \\\n",
    "    \"Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.\" \\\n",
    "    \"Output the bbox(es) and point(s) inside the interested object(s) in JSON format.\" \\\n",
    "    \"i.e., <think> thinking process here </think>\" \\\n",
    "    \"<answer>{Answer}</answer>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dd73ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: 1068 x 1137\n",
      "x_factor: 1.2714285714285714 y_factor: 1.3535714285714286\n"
     ]
    }
   ],
   "source": [
    "text = \"Which plant is taller?\"\n",
    "image_path = \"../samples/plants.jpg\"\n",
    "\n",
    "image = PILImage.open(image_path).convert(\"RGB\")\n",
    "original_width, original_height = image.size\n",
    "resize_size = 840\n",
    "x_factor, y_factor = original_width/resize_size, original_height/resize_size\n",
    "print(f\"Original image size: {original_width} x {original_height}\")\n",
    "print(\"x_factor:\", x_factor, \"y_factor:\", y_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a769c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = []\n",
    "message = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "    {\n",
    "        \"type\": \"image\", \n",
    "        \"image\": image.resize((resize_size, resize_size), PILImage.BILINEAR)\n",
    "    },\n",
    "    {   \n",
    "        \"type\": \"text\",\n",
    "        \"text\": QUESTION_TEMPLATE.format(\n",
    "            Question=text.lower().strip(\".\"),\n",
    "            Answer=(\n",
    "                \"[{\\\"bbox_2d\\\": [10,100,200,210], \\\"point_2d\\\": [30,110]}, {\\\"bbox_2d\\\": \"\n",
    "                \"[225,296,706,786], \\\"point_2d\\\": [302,410]}]\"\n",
    "            )\n",
    "        )    \n",
    "    }\n",
    "]\n",
    "}]\n",
    "messages.append(message)\n",
    "\n",
    "# Preparation for inference\n",
    "text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "#pdb.set_trace()\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "#pdb.set_trace()\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c92458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bbox_points_think(output_text, x_factor, y_factor):\n",
    "    json_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', output_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        data = json.loads(json_match.group(1))\n",
    "        pred_bboxes = [[\n",
    "            int(item['bbox_2d'][0] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][1] * y_factor + 0.5),\n",
    "            int(item['bbox_2d'][2] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][3] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "        pred_points = [[\n",
    "            int(item['point_2d'][0] * x_factor + 0.5),\n",
    "            int(item['point_2d'][1] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "    \n",
    "    think_pattern = r'<think>([^<]+)</think>'\n",
    "    think_match = re.search(think_pattern, output_text)\n",
    "    think_text = \"\"\n",
    "    if think_match:\n",
    "        think_text = think_match.group(1)\n",
    "    \n",
    "    return pred_bboxes, pred_points, think_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d287807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> To determine which plant is taller, I need to compare the height of the plants relative to their pots. The plant on the right appears to have a taller stem and larger leaves, suggesting it is taller than the plant on the left. The pots themselves are the same height, so the difference in height is due to the plants themselves.</think>\n",
      "<answer>[{\"bbox_2d\": [337,56,620,587], \"point_2d\": [510,280]}, {\"bbox_2d\": [107,354,364,664], \"point_2d\": [218,470]}]</answer>\n",
      "[[648, 379], [277, 636]] 2\n",
      "Thinking process:   To determine which plant is taller, I need to compare the height of the plants relative to their pots. The plant on the right appears to have a taller stem and larger leaves, suggesting it is taller than the plant on the left. The pots themselves are the same height, so the difference in height is due to the plants themselves.\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = reasoning_model.generate(\n",
    "    **inputs, use_cache=True, max_new_tokens=1024, do_sample=False\n",
    ")\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text[0])\n",
    "# pdb.set_trace()\n",
    "bboxes, points, think = extract_bbox_points_think(output_text[0], x_factor, y_factor)\n",
    "print(points, len(points))\n",
    "\n",
    "print(\"Thinking process: \", think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fce37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segzero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
