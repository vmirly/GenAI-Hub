{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27436cde",
   "metadata": {},
   "source": [
    "AnomalyCLIP\n",
    "======\n",
    "\n",
    " **AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection**\n",
    "\n",
    "* Paper: https://arxiv.org/pdf/2310.18961\n",
    "\n",
    "![AnomalyCLIP overview](../assets/anomalyclip_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31c68f",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/zqhang/AnomalyCLIP.git AnomalyCLIP_repo\n",
    "\n",
    "conda create --name anomalyclip python=3.10 -y\n",
    "conda activate anomalyclip\n",
    "\n",
    "pip install -r AnomalyCLIP_repo/requirements.txt \n",
    "pip install thop ftfy regex tabulate opencv-python\n",
    "\n",
    "pip install \"numpy<2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92adeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/anomalyclip/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "sys.path.append(\"AnomalyCLIP_repo\")\n",
    "import AnomalyCLIP_lib\n",
    "from prompt_ensemble import AnomalyCLIP_PromptLearner\n",
    "from loss import FocalLoss, BinaryDiceLoss\n",
    "from utils import normalize\n",
    "from dataset import Dataset\n",
    "from logger import get_logger\n",
    "from utils import get_transform\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from visualization import visualizer\n",
    "from metrics import image_level_metrics, pixel_level_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a7d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name ViT-L/14@336px\n",
      "text_layer False\n",
      "text_layer True\n"
     ]
    }
   ],
   "source": [
    "n_ctx = 12\n",
    "depth = 9\n",
    "t_n_ctx = 4\n",
    "image_size = 518\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "AnomalyCLIP_parameters = {\n",
    "    \"Prompt_length\": n_ctx,\n",
    "    \"learnabel_text_embedding_depth\": depth,\n",
    "    \"learnabel_text_embedding_length\": t_n_ctx\n",
    "}\n",
    "\n",
    "model, _ = AnomalyCLIP_lib.load(\n",
    "    \"ViT-L/14@336px\",\n",
    "    device=device,\n",
    "    design_details=AnomalyCLIP_parameters\n",
    ")\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25190c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from AnomalyCLIP_lib.transform import image_transform\n",
    "from AnomalyCLIP_lib.constants import (\n",
    "    OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n",
    ")\n",
    "\n",
    "def get_transform(image_size,):\n",
    "    preprocess = image_transform(\n",
    "        image_size,\n",
    "        is_train=False,\n",
    "        mean=OPENAI_DATASET_MEAN,\n",
    "        std=OPENAI_DATASET_STD\n",
    "    )\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    preprocess.transforms[0] = transforms.Resize(\n",
    "        size=(image_size, image_size),\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "        max_size=None,\n",
    "        antialias=None\n",
    "    )\n",
    "    preprocess.transforms[1] = transforms.CenterCrop(\n",
    "        size=(image_size, image_size)\n",
    "    )\n",
    "    return preprocess, target_transform\n",
    "\n",
    "\n",
    "preprocess, target_transform = get_transform(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bea47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalyclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
