{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843ea22d",
   "metadata": {},
   "source": [
    "GLIDE\n",
    "====\n",
    "\n",
    "**GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**\n",
    "\n",
    " * Paper: https://arxiv.org/abs/2112.10741\n",
    " * Code: https://github.com/openai/glide-text2im\n",
    "\n",
    "![GLIDE Examples](../assets/glide_examples.jpg)\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create -n glide python=3.11 -y\n",
    "conda activate glide\n",
    "pip install git+https://github.com/openai/glide-text2im\n",
    "pip install pyyaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe7693",
   "metadata": {},
   "source": [
    "## text-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/glide/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    "    model_and_diffusion_defaults_upsampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f95eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.54G/1.54G [01:56<00:00, 13.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * total base parameters 385030726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.59G/1.59G [02:01<00:00, 13.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * total upsampler parameters 398361286\n"
     ]
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "base_steps = \"100\"  # number of diffusion steps to use for sampling\n",
    "upsample_steps = \"fast27\"  # number of diffusion steps to use for upsampling\n",
    "device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "\n",
    "# Create base model.\n",
    "options = model_and_diffusion_defaults()\n",
    "options['use_fp16'] = has_cuda\n",
    "options['timestep_respacing'] = str(base_steps)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**options)\n",
    "model.eval()\n",
    "if has_cuda:\n",
    "    model.convert_to_fp16()\n",
    "model.to(device)\n",
    "model.load_state_dict(load_checkpoint('base', device))\n",
    "print(' * total base parameters', sum(x.numel() for x in model.parameters()))\n",
    "\n",
    "\n",
    "\n",
    "# Create upsampler model.\n",
    "options_up = model_and_diffusion_defaults_upsampler()\n",
    "options_up['use_fp16'] = has_cuda\n",
    "options_up['timestep_respacing'] = upsample_steps\n",
    "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
    "model_up.eval()\n",
    "if has_cuda:\n",
    "    model_up.convert_to_fp16()\n",
    "model_up.to(device)\n",
    "model_up.load_state_dict(load_checkpoint('upsample', device))\n",
    "print(' * total upsampler parameters', sum(x.numel() for x in model_up.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0e1f8",
   "metadata": {},
   "source": [
    "### Helper function: Sample from base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13cfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Sample from the base model #\n",
    "##############################\n",
    " \n",
    "def sample_base(prompt, batch_size=1, guidance_scale=3.0):\n",
    "    # Create the text tokens to feed to the model.\n",
    "    tokens = model.tokenizer.encode(prompt)\n",
    "    tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
    "        tokens, options['text_ctx']\n",
    "    )\n",
    "\n",
    "    # Create the classifier-free guidance tokens (empty)\n",
    "    full_batch_size = batch_size * 2\n",
    "    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
    "        [], options['text_ctx']\n",
    "    )\n",
    "\n",
    "    # Pack the tokens together into model kwargs.\n",
    "    model_kwargs = dict(\n",
    "        tokens=torch.tensor(\n",
    "            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
    "        ),\n",
    "        mask=torch.tensor(\n",
    "            [mask] * batch_size + [uncond_mask] * batch_size,\n",
    "            dtype=torch.bool,\n",
    "            device=device,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Create a classifier-free guidance sampling function\n",
    "    def model_fn(x_t, ts, **kwargs):\n",
    "        half = x_t[: len(x_t) // 2]\n",
    "        combined = torch.cat([half, half], dim=0)\n",
    "        model_out = model(combined, ts, **kwargs)\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
    "        return torch.cat([eps, rest], dim=1)\n",
    "\n",
    "    # Sample from the base model.\n",
    "    model.del_cache()\n",
    "    samples = diffusion.p_sample_loop(\n",
    "        model_fn,\n",
    "        (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
    "        device=device,\n",
    "        clip_denoised=True,\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        cond_fn=None,\n",
    "    )[:batch_size]\n",
    "    model.del_cache()\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0dd71",
   "metadata": {},
   "source": [
    "### Helper function: Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad9551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Upsample the 64x64 samples #\n",
    "##############################\n",
    "\n",
    "def upsample(samples, prompt, batch_size=1, upsample_temp=0.997):\n",
    "    tokens = model_up.tokenizer.encode(prompt)\n",
    "    tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
    "        tokens, options_up['text_ctx']\n",
    "    )\n",
    "\n",
    "    # Create the model conditioning dict.\n",
    "    model_kwargs = dict(\n",
    "        # Low-res image to upsample.\n",
    "        low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
    "\n",
    "        # Text tokens\n",
    "        tokens=torch.tensor(\n",
    "            [tokens] * batch_size, device=device\n",
    "        ),\n",
    "        mask=torch.tensor(\n",
    "            [mask] * batch_size,\n",
    "            dtype=torch.bool,\n",
    "            device=device,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Sample from the base model.\n",
    "    model_up.del_cache()\n",
    "    up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
    "    up_samples = diffusion_up.ddim_sample_loop(\n",
    "        model_up,\n",
    "        up_shape,\n",
    "        noise=torch.randn(up_shape, device=device) * upsample_temp,\n",
    "        device=device,\n",
    "        clip_denoised=True,\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        cond_fn=None,\n",
    "    )[:batch_size]\n",
    "    model_up.del_cache()\n",
    "\n",
    "    return up_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3df69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
