{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b22b9b2",
   "metadata": {},
   "source": [
    "UNO\n",
    "====\n",
    "\n",
    "**Less-to-More Generalization: Unlocking More Controllability by In-Context Generation**\n",
    "\n",
    " * Paper: https://arxiv.org/abs/2504.02160\n",
    "\n",
    "![UNO Overview](../assets/uno_overview.png)\n",
    "\n",
    "\n",
    "```bash\n",
    "# installation\n",
    "git clone https://github.com/bytedance/UNO.git\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e49499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/py311/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init AE\n",
      "Init model\n",
      "Loading lora\n",
      "Loading main checkpoint\n",
      "####\n",
      "We are in fp8 mode right now, since the fp8 checkpoint of XLabs-AI/flux-dev-fp8 seems broken\n",
      "we convert the fp8 checkpoint on flight from bf16 checkpoint\n",
      "If your storage is constrainedyou can save the fp8 checkpoint and replace the bf16 checkpoint by yourself\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<uno.flux.pipeline.UNOPipeline at 0x7f107b760f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import dataclasses\n",
    "from typing import Literal\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import HfArgumentParser\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "sys.path.append(\"./UNO\")\n",
    "from uno.flux.pipeline import UNOPipeline, preprocess_ref\n",
    "\n",
    "\n",
    "def horizontal_concat(images):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        new_im.paste(img, (x_offset, 0))\n",
    "        x_offset += img.size[0]\n",
    "\n",
    "    return new_im\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class InferenceArgs:\n",
    "    prompt: str | None = None\n",
    "    image_paths: list[str] | None = None\n",
    "    eval_json_path: str | None = None\n",
    "    offload: bool = False\n",
    "    num_images_per_prompt: int = 3\n",
    "    model_type: Literal[\"flux-dev\", \"flux-dev-fp8\", \"flux-schnell\"] = \"flux-dev-fp8\"\n",
    "    width: int = 256\n",
    "    height: int = 256\n",
    "    ref_size: int = -1\n",
    "    num_steps: int = 25\n",
    "    guidance: float = 4\n",
    "    seed: int = 3407\n",
    "    save_path: str = \"output/inference\"\n",
    "    only_lora: bool = True\n",
    "    concat_refs: bool = False\n",
    "    lora_rank: int = 512\n",
    "    data_resolution: int = 256\n",
    "    pe: Literal['d', 'h', 'w', 'o'] = 'd'\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# build model args:\n",
    "parser = HfArgumentParser([InferenceArgs])\n",
    "model_args = parser.parse_args_into_dataclasses(args=[])[0]\n",
    "\n",
    "# load pipeline:\n",
    "pipeline = UNOPipeline(\n",
    "    model_args.model_type,\n",
    "    accelerator.device,\n",
    "    model_args.offload,\n",
    "    only_lora=model_args.only_lora,\n",
    "    lora_rank=model_args.lora_rank\n",
    ")\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56db83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(pipeline, image_paths, prompt):\n",
    "    data_root = \"./\"\n",
    "    data_dicts = [{\"prompt\": prompt, \"image_paths\": image_paths}]\n",
    "\n",
    "    for (i, data_dict), j in itertools.product(\n",
    "            enumerate(data_dicts),\n",
    "            range(model_args.num_images_per_prompt)\n",
    "        ):\n",
    "        if (i * model_args.num_images_per_prompt + j) % accelerator.num_processes != accelerator.process_index:\n",
    "            continue\n",
    "\n",
    "        ref_imgs = [\n",
    "            Image.open(os.path.join(data_root, img_path))\n",
    "            for img_path in data_dict[\"image_paths\"]\n",
    "        ]\n",
    "        if model_args.ref_size==-1:\n",
    "            model_args.ref_size = 512 if len(ref_imgs)==1 else 320\n",
    "\n",
    "        ref_imgs = [preprocess_ref(img, model_args.ref_size) for img in ref_imgs]\n",
    "\n",
    "        image_gen = pipeline(\n",
    "            prompt=data_dict[\"prompt\"],\n",
    "            width=model_args.width,\n",
    "            height=model_args.height,\n",
    "            guidance=model_args.guidance,\n",
    "            num_steps=model_args.num_steps,\n",
    "            seed=model_args.seed + j,\n",
    "            ref_imgs=ref_imgs,\n",
    "            pe=model_args.pe,\n",
    "        )\n",
    "        if model_args.concat_refs:\n",
    "            image_gen = horizontal_concat([image_gen, *ref_imgs])\n",
    "\n",
    "        plt.imshow(image_gen)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c11159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
