{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcca211",
   "metadata": {},
   "source": [
    "VisionReasoner\n",
    "====\n",
    "\n",
    " **VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning**\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2505.12081\n",
    "\n",
    "![VisionReasoner Overview](../assets/visionreasoner_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e5157",
   "metadata": {},
   "source": [
    "* Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dvlab-research/VisionReasoner.git VisionReasoner_repo\n",
    "cd VisionReasoner_repo\n",
    "conda create -n visionreasoner_test python=3.12\n",
    "conda activate visionreasoner_test\n",
    "pip3 install torch torchvision\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "* FlashAttention\n",
    "If you dont have flash-attention installed, and don't want to install it, change the following lines to `\"eager\"`\n",
    "\n",
    "```\n",
    "# Line 63 - File: VisionReasoner_repo/vision_reasoner/models/vision_reasoner_model.py\n",
    "attn_implementation=\"flash_attention_2\", # -> \"eager\"\n",
    "\n",
    "# Line 11 - File VisionReasoner_repo/vision_reasoner/models/task_router.py\n",
    "attn_implementation=\"flash_attention_2\", # -> \"eager\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2e00d",
   "metadata": {},
   "source": [
    " * Download the model\n",
    "\n",
    "```bash\n",
    "mkdir pretrained_models\n",
    "cd pretrained_models\n",
    "git lfs install\n",
    "git clone https://huggingface.co/Ricky06662/VisionReasoner-7B\n",
    "git clone https://huggingface.co/Ricky06662/TaskRouter-1.5B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7baaba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/visionreasoner_test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "sys.path.append(\"VisionReasoner_repo/vision_reasoner\")\n",
    "from models.vision_reasoner_model import VisionReasonerModel\n",
    "from utils import (\n",
    "    visualize_results_enhanced,\n",
    "    visualize_pose_estimation_results_enhanced,\n",
    "    visualize_depth_estimation_results_enhanced,\n",
    "    visualize_results_video,\n",
    "    visualize_pose_estimation_results_video,\n",
    "    visualize_depth_estimation_results_video\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48927cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'pretrained_models/VisionReasoner-7B'\n",
    "task_router_model_path = \"pretrained_models/TaskRouter-1.5B\"\n",
    "segmentation_model_path = \"facebook/sam2-hiera-large\"\n",
    "\n",
    "task_type = \"auto\"\n",
    "# Possible choices for task_type:\n",
    "#  - \"auto\"\n",
    "#  - \"detection\"\n",
    "#  - \"segmentation\"\n",
    "#  - \"counting\"\n",
    "#  - \"vqa\"\n",
    "#  - \"generation\"\n",
    "#  - \"depth_estimation\"\n",
    "#  - \"pose_estimation\"\n",
    "\n",
    "hybrid_mode = False\n",
    "yolo_model_path = \"yolov8x-worldv2.pt\"\n",
    "refer_image_path = \"\"\n",
    "image_prompt = \"\"\n",
    "generation_mode = False\n",
    "generation_model_name = \"gpt-image-1\"\n",
    "depth_estimation_model_path = None  # facebook/VGGT-1B\n",
    "pose_estimation_model_path = None  # usyd-community/vitpose-plus-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b15272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "model = VisionReasonerModel(\n",
    "    reasoning_model_path=model_path, \n",
    "    task_router_model_path=task_router_model_path, \n",
    "    segmentation_model_path=segmentation_model_path,\n",
    "    depth_estimation_model_path=depth_estimation_model_path,\n",
    "    pose_estimation_model_path=pose_estimation_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70c951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionreasoner_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
