{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcca211",
   "metadata": {},
   "source": [
    "VisionReasoner\n",
    "====\n",
    "\n",
    " **VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning**\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2505.12081\n",
    "\n",
    "![VisionReasoner Overview](../assets/visionreasoner_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e5157",
   "metadata": {},
   "source": [
    "* Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dvlab-research/VisionReasoner.git VisionReasoner_repo\n",
    "cd VisionReasoner_repo\n",
    "conda create -n visionreasoner_test python=3.12\n",
    "conda activate visionreasoner_test\n",
    "pip3 install torch torchvision\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "* FlashAttention\n",
    "If you dont have flash-attention installed, and don't want to install it, change the following lines to `\"eager\"`\n",
    "\n",
    "```\n",
    "# Line 63 - File: VisionReasoner_repo/vision_reasoner/models/vision_reasoner_model.py\n",
    "attn_implementation=\"flash_attention_2\", # -> \"eager\"\n",
    "\n",
    "# Line 11 - File VisionReasoner_repo/vision_reasoner/models/task_router.py\n",
    "attn_implementation=\"flash_attention_2\", # -> \"eager\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2e00d",
   "metadata": {},
   "source": [
    " * Download the model\n",
    "\n",
    "```bash\n",
    "mkdir pretrained_models\n",
    "cd pretrained_models\n",
    "git lfs install\n",
    "git clone https://huggingface.co/Ricky06662/VisionReasoner-7B\n",
    "git clone https://huggingface.co/Ricky06662/TaskRouter-1.5B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baaba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyml/anaconda3/envs/visionreasoner_test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "sys.path.append(\"VisionReasoner_repo/vision_reasoner\")\n",
    "from models.vision_reasoner_model import VisionReasonerModel\n",
    "from utils import (\n",
    "    visualize_results_enhanced,\n",
    "    visualize_pose_estimation_results_enhanced,\n",
    "    visualize_depth_estimation_results_enhanced,\n",
    "    visualize_results_video,\n",
    "    visualize_pose_estimation_results_video,\n",
    "    visualize_depth_estimation_results_video\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48927cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'pretrained_models/VisionReasoner-7B'\n",
    "task_router_model_path = \"pretrained_models/TaskRouter-1.5B\"\n",
    "segmentation_model_path = \"facebook/sam2-hiera-large\"\n",
    "\n",
    "task_type = \"auto\"\n",
    "# Possible choices for task_type:\n",
    "#  - \"auto\"\n",
    "#  - \"detection\"\n",
    "#  - \"segmentation\"\n",
    "#  - \"counting\"\n",
    "#  - \"vqa\"\n",
    "#  - \"generation\"\n",
    "#  - \"depth_estimation\"\n",
    "#  - \"pose_estimation\"\n",
    "\n",
    "hybrid_mode = False\n",
    "yolo_model_path = \"yolov8x-worldv2.pt\"\n",
    "generation_mode = False\n",
    "generation_model_name = \"gpt-image-1\"\n",
    "depth_estimation_model_path = None  # facebook/VGGT-1B\n",
    "pose_estimation_model_path = None  # usyd-community/vitpose-plus-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b15272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "model = VisionReasonerModel(\n",
    "    reasoning_model_path=model_path, \n",
    "    task_router_model_path=task_router_model_path, \n",
    "    segmentation_model_path=segmentation_model_path,\n",
    "    depth_estimation_model_path=depth_estimation_model_path,\n",
    "    pose_estimation_model_path=pose_estimation_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd70c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(image, task_type, query, refer_image_path=\"\", image_prompt=\"\"):\n",
    "    if task_type == \"auto\":\n",
    "        result, task_type = model.process_single_image(image, query, return_task_type=True)\n",
    "    elif task_type == \"detection\":\n",
    "        result = model.detect_objects(image, query)\n",
    "    elif task_type == \"segmentation\":\n",
    "        result = model.segment_objects(image, query)\n",
    "    elif task_type == \"counting\":\n",
    "        result = model.count_objects(image, query)\n",
    "    elif task_type == \"generation\":\n",
    "        result = model.generate_image(refer_image_path, image_prompt)\n",
    "    elif task_type == \"depth_estimation\":\n",
    "        result = model.depth_estimation(image, query)\n",
    "    elif task_type == \"pose_estimation\":\n",
    "        result = model.pose_estimation(image, query)\n",
    "    else:    # VQA\n",
    "        result = model.answer_question(image, query)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05803a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['count', 'bboxes', 'points', 'thinking', 'full_response', 'pred_answer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"../samples/plants.jpg\"\n",
    "query = \"How many plants are there in this image?\"\n",
    "refer_image_path = \"\"\n",
    "image_prompt = \"\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "result = inference(\n",
    "    image=image,\n",
    "    task_type=task_type,\n",
    "    query=query,\n",
    "    refer_image_path=refer_image_path,\n",
    "    image_prompt=image_prompt\n",
    ")\n",
    "\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79541571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The task involves identifying the number of plants in the image and comparing\n",
      "the objects to find the most closely matched ones. I'll start through the image\n",
      "step by step:  1. First, I'll identify the plants in the image. There are two\n",
      "distinct plants visible. 2. Next to each plant, there is a gold-colored metal\n",
      "stand that appears to be a stand holder or stand stand. 3. The plants are are in\n",
      "black pots, which are are are not the main focus of the image but but are are\n",
      "are part of the plants. 4. The metal holders are are the most closely matched\n",
      "objects to the question, as they are are the main focus of the image and the\n",
      "question asks about the number of plants.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(result[\"thinking\"], width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8ef296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> The task involves identifying the number of plants in the image and\n",
      "comparing the objects to find the most closely matched ones. I'll start through\n",
      "the image step by step:  1. First, I'll identify the plants in the image. There\n",
      "are two distinct plants visible. 2. Next to each plant, there is a gold-colored\n",
      "metal stand that appears to be a stand holder or stand stand. 3. The plants are\n",
      "are in black pots, which are are are not the main focus of the image but but are\n",
      "are are part of the plants. 4. The metal holders are are the most closely\n",
      "matched objects to the question, as they are are the main focus of the image and\n",
      "the question asks about the number of plants.  </think> <answer>[{\"bbox_2d\":\n",
      "[108,354,364,518], \"point_2d\": [218,468]}, {\"bbox_2d\": [337,56,620,412],\n",
      "\"point_2d\": [515,280]}]</answer>\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result[\"full_response\"], width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098d6147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bbox_2d': [108, 354, 364, 518], 'point_2d': [218, 468]},\n",
       " {'bbox_2d': [337, 56, 620, 412], 'point_2d': [515, 280]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"pred_answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033111f0",
   "metadata": {},
   "source": [
    "### task_type=\"segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013d038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['count', 'bboxes', 'points', 'thinking', 'full_response', 'pred_answer'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = inference(\n",
    "    image=image,\n",
    "    task_type=task_type,\n",
    "    query=query,\n",
    "    refer_image_path=refer_image_path,\n",
    "    image_prompt=image_prompt\n",
    ")\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0efbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 2,\n",
       " 'bboxes': [[137, 479, 463, 701], [428, 76, 788, 558]],\n",
       " 'points': [[277, 633], [655, 379]],\n",
       " 'thinking': \" The task involves identifying the number of plants in the image and comparing the objects to find the most closely matched ones. I'll start through the image step by step:\\n\\n1. First, I'll identify the plants in the image. There are two distinct plants visible.\\n2. Next to each plant, there is a gold-colored metal stand that appears to be a stand holder or stand stand.\\n3. The plants are are in black pots, which are are are not the main focus of the image but but are are are part of the plants.\\n4. The metal holders are are the most closely matched objects to the question, as they are are the main focus of the image and the question asks about the number of plants.\\n\\n\",\n",
       " 'full_response': '<think> The task involves identifying the number of plants in the image and comparing the objects to find the most closely matched ones. I\\'ll start through the image step by step:\\n\\n1. First, I\\'ll identify the plants in the image. There are two distinct plants visible.\\n2. Next to each plant, there is a gold-colored metal stand that appears to be a stand holder or stand stand.\\n3. The plants are are in black pots, which are are are not the main focus of the image but but are are are part of the plants.\\n4. The metal holders are are the most closely matched objects to the question, as they are are the main focus of the image and the question asks about the number of plants.\\n\\n</think>\\n<answer>[{\"bbox_2d\": [108,354,364,518], \"point_2d\": [218,468]}, {\"bbox_2d\": [337,56,620,412], \"point_2d\": [515,280]}]</answer>',\n",
       " 'pred_answer': [{'bbox_2d': [108, 354, 364, 518], 'point_2d': [218, 468]},\n",
       "  {'bbox_2d': [337, 56, 620, 412], 'point_2d': [515, 280]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92da4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionreasoner_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
